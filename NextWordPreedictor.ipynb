{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0lGEVATdxFGItGKgb3zXy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SQexMuCDatUk"
      },
      "outputs": [],
      "source": [
        "faqs=\"\"\"\n",
        "Meet Pranjal, a captivating blend of grace and charisma. At 21, she exudes a magnetic\n",
        "energy that draws people in effortlessly. With an air of independence, Pranjal navigates\n",
        "life with an inspiring confidence, her every step a testament to her unwavering determination.\n",
        "Her beautiful locks cascade like a waterfall of silk, framing a smile that lights up the darkest of rooms.\n",
        "But it's not just her outer beauty that captivates; it's her inner radiance that truly shines.\n",
        "Pranjal possesses a rare maturity and understanding far beyond her years, offering wisdom\n",
        "and guidance with a gentle touch. Her dedication to hard work is unparalleled, yet she always\n",
        "finds time to spread joy and uplift those around her. Whether lending a listening ear or offering\n",
        "sage advice, Pranjal is a beacon of positivity and encouragement. In her presence, one can't help but\n",
        "feel inspired to reach for the stars.\n",
        "Pranjal's pursuit of excellence extends beyond her personal achievements; she's a shining\n",
        "star in academia, a beacon of inspiration for both parents and teachers alike. Her dedication\n",
        "to her studies is unwavering, earning her the admiration and pride of those who know her best.\n",
        "While her circle of friends may be small, it's filled with genuine connections forged through trust and\n",
        "loyalty. Pranjal values quality over quantity, cherishing the true friendships that enrich her life.\n",
        "In the eyes of many, she epitomizes perfection, a testament to her remarkable blend of\n",
        "intelligence, compassion, and integrity. Pranjal isn't just the perfect girl in my view;\n",
        "she's a rare gem in a world craving authenticity and sincerity.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "rgJx7hLJbREm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()"
      ],
      "metadata": {
        "id": "cIny00M-beVM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([faqs])"
      ],
      "metadata": {
        "id": "V7T5qZeTbhsd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE8v0LgzboOi",
        "outputId": "b7a04cb0-cd84-49b0-a50c-adbad91cc08e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'her': 1,\n",
              " 'a': 2,\n",
              " 'of': 3,\n",
              " 'and': 4,\n",
              " 'pranjal': 5,\n",
              " 'in': 6,\n",
              " 'to': 7,\n",
              " 'the': 8,\n",
              " 'that': 9,\n",
              " 'with': 10,\n",
              " 'she': 11,\n",
              " \"it's\": 12,\n",
              " 'is': 13,\n",
              " 'blend': 14,\n",
              " 'an': 15,\n",
              " 'life': 16,\n",
              " 'testament': 17,\n",
              " 'unwavering': 18,\n",
              " 'but': 19,\n",
              " 'just': 20,\n",
              " 'rare': 21,\n",
              " 'beyond': 22,\n",
              " 'offering': 23,\n",
              " 'dedication': 24,\n",
              " 'those': 25,\n",
              " 'beacon': 26,\n",
              " 'for': 27,\n",
              " \"she's\": 28,\n",
              " 'meet': 29,\n",
              " 'captivating': 30,\n",
              " 'grace': 31,\n",
              " 'charisma': 32,\n",
              " 'at': 33,\n",
              " '21': 34,\n",
              " 'exudes': 35,\n",
              " 'magnetic': 36,\n",
              " 'energy': 37,\n",
              " 'draws': 38,\n",
              " 'people': 39,\n",
              " 'effortlessly': 40,\n",
              " 'air': 41,\n",
              " 'independence': 42,\n",
              " 'navigates': 43,\n",
              " 'inspiring': 44,\n",
              " 'confidence': 45,\n",
              " 'every': 46,\n",
              " 'step': 47,\n",
              " 'determination': 48,\n",
              " 'beautiful': 49,\n",
              " 'locks': 50,\n",
              " 'cascade': 51,\n",
              " 'like': 52,\n",
              " 'waterfall': 53,\n",
              " 'silk': 54,\n",
              " 'framing': 55,\n",
              " 'smile': 56,\n",
              " 'lights': 57,\n",
              " 'up': 58,\n",
              " 'darkest': 59,\n",
              " 'rooms': 60,\n",
              " 'not': 61,\n",
              " 'outer': 62,\n",
              " 'beauty': 63,\n",
              " 'captivates': 64,\n",
              " 'inner': 65,\n",
              " 'radiance': 66,\n",
              " 'truly': 67,\n",
              " 'shines': 68,\n",
              " 'possesses': 69,\n",
              " 'maturity': 70,\n",
              " 'understanding': 71,\n",
              " 'far': 72,\n",
              " 'years': 73,\n",
              " 'wisdom': 74,\n",
              " 'guidance': 75,\n",
              " 'gentle': 76,\n",
              " 'touch': 77,\n",
              " 'hard': 78,\n",
              " 'work': 79,\n",
              " 'unparalleled': 80,\n",
              " 'yet': 81,\n",
              " 'always': 82,\n",
              " 'finds': 83,\n",
              " 'time': 84,\n",
              " 'spread': 85,\n",
              " 'joy': 86,\n",
              " 'uplift': 87,\n",
              " 'around': 88,\n",
              " 'whether': 89,\n",
              " 'lending': 90,\n",
              " 'listening': 91,\n",
              " 'ear': 92,\n",
              " 'or': 93,\n",
              " 'sage': 94,\n",
              " 'advice': 95,\n",
              " 'positivity': 96,\n",
              " 'encouragement': 97,\n",
              " 'presence': 98,\n",
              " 'one': 99,\n",
              " \"can't\": 100,\n",
              " 'help': 101,\n",
              " 'feel': 102,\n",
              " 'inspired': 103,\n",
              " 'reach': 104,\n",
              " 'stars': 105,\n",
              " \"pranjal's\": 106,\n",
              " 'pursuit': 107,\n",
              " 'excellence': 108,\n",
              " 'extends': 109,\n",
              " 'personal': 110,\n",
              " 'achievements': 111,\n",
              " 'shining': 112,\n",
              " 'star': 113,\n",
              " 'academia': 114,\n",
              " 'inspiration': 115,\n",
              " 'both': 116,\n",
              " 'parents': 117,\n",
              " 'teachers': 118,\n",
              " 'alike': 119,\n",
              " 'studies': 120,\n",
              " 'earning': 121,\n",
              " 'admiration': 122,\n",
              " 'pride': 123,\n",
              " 'who': 124,\n",
              " 'know': 125,\n",
              " 'best': 126,\n",
              " 'while': 127,\n",
              " 'circle': 128,\n",
              " 'friends': 129,\n",
              " 'may': 130,\n",
              " 'be': 131,\n",
              " 'small': 132,\n",
              " 'filled': 133,\n",
              " 'genuine': 134,\n",
              " 'connections': 135,\n",
              " 'forged': 136,\n",
              " 'through': 137,\n",
              " 'trust': 138,\n",
              " 'loyalty': 139,\n",
              " 'values': 140,\n",
              " 'quality': 141,\n",
              " 'over': 142,\n",
              " 'quantity': 143,\n",
              " 'cherishing': 144,\n",
              " 'true': 145,\n",
              " 'friendships': 146,\n",
              " 'enrich': 147,\n",
              " 'eyes': 148,\n",
              " 'many': 149,\n",
              " 'epitomizes': 150,\n",
              " 'perfection': 151,\n",
              " 'remarkable': 152,\n",
              " 'intelligence': 153,\n",
              " 'compassion': 154,\n",
              " 'integrity': 155,\n",
              " \"isn't\": 156,\n",
              " 'perfect': 157,\n",
              " 'girl': 158,\n",
              " 'my': 159,\n",
              " 'view': 160,\n",
              " 'gem': 161,\n",
              " 'world': 162,\n",
              " 'craving': 163,\n",
              " 'authenticity': 164,\n",
              " 'sincerity': 165}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences=[]\n",
        "for sentence in faqs.split('\\n'):\n",
        "  # print(sentence)\n",
        "  tokenized_sentence=tokenizer.texts_to_sequences([sentence])[0]\n",
        "  print(tokenized_sentence)\n",
        "  for i in range(1,len(tokenized_sentence)):\n",
        "    input_sequences.append(tokenized_sentence[:i+1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59h3g1g_br42",
        "outputId": "f69ea6f1-1b8a-4737-da5e-24e9c76f90ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[29, 5, 2, 30, 14, 3, 31, 4, 32, 33, 34, 11, 35, 2, 36]\n",
            "[37, 9, 38, 39, 6, 40, 10, 15, 41, 3, 42, 5, 43]\n",
            "[16, 10, 15, 44, 45, 1, 46, 47, 2, 17, 7, 1, 18, 48]\n",
            "[1, 49, 50, 51, 52, 2, 53, 3, 54, 55, 2, 56, 9, 57, 58, 8, 59, 3, 60]\n",
            "[19, 12, 61, 20, 1, 62, 63, 9, 64, 12, 1, 65, 66, 9, 67, 68]\n",
            "[5, 69, 2, 21, 70, 4, 71, 72, 22, 1, 73, 23, 74]\n",
            "[4, 75, 10, 2, 76, 77, 1, 24, 7, 78, 79, 13, 80, 81, 11, 82]\n",
            "[83, 84, 7, 85, 86, 4, 87, 25, 88, 1, 89, 90, 2, 91, 92, 93, 23]\n",
            "[94, 95, 5, 13, 2, 26, 3, 96, 4, 97, 6, 1, 98, 99, 100, 101, 19]\n",
            "[102, 103, 7, 104, 27, 8, 105]\n",
            "[106, 107, 3, 108, 109, 22, 1, 110, 111, 28, 2, 112]\n",
            "[113, 6, 114, 2, 26, 3, 115, 27, 116, 117, 4, 118, 119, 1, 24]\n",
            "[7, 1, 120, 13, 18, 121, 1, 8, 122, 4, 123, 3, 25, 124, 125, 1, 126]\n",
            "[127, 1, 128, 3, 129, 130, 131, 132, 12, 133, 10, 134, 135, 136, 137, 138, 4]\n",
            "[139, 5, 140, 141, 142, 143, 144, 8, 145, 146, 9, 147, 1, 16]\n",
            "[6, 8, 148, 3, 149, 11, 150, 151, 2, 17, 7, 1, 152, 14, 3]\n",
            "[153, 154, 4, 155, 5, 156, 20, 8, 157, 158, 6, 159, 160]\n",
            "[28, 2, 21, 161, 6, 2, 162, 163, 164, 4, 165]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQOvAY5Oc0TB",
        "outputId": "12076d1e-086a-43cd-9874-7200f15f406a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[29, 5],\n",
              " [29, 5, 2],\n",
              " [29, 5, 2, 30],\n",
              " [29, 5, 2, 30, 14],\n",
              " [29, 5, 2, 30, 14, 3],\n",
              " [29, 5, 2, 30, 14, 3, 31],\n",
              " [29, 5, 2, 30, 14, 3, 31, 4],\n",
              " [29, 5, 2, 30, 14, 3, 31, 4, 32],\n",
              " [29, 5, 2, 30, 14, 3, 31, 4, 32, 33],\n",
              " [29, 5, 2, 30, 14, 3, 31, 4, 32, 33, 34],\n",
              " [29, 5, 2, 30, 14, 3, 31, 4, 32, 33, 34, 11],\n",
              " [29, 5, 2, 30, 14, 3, 31, 4, 32, 33, 34, 11, 35],\n",
              " [29, 5, 2, 30, 14, 3, 31, 4, 32, 33, 34, 11, 35, 2],\n",
              " [29, 5, 2, 30, 14, 3, 31, 4, 32, 33, 34, 11, 35, 2, 36],\n",
              " [37, 9],\n",
              " [37, 9, 38],\n",
              " [37, 9, 38, 39],\n",
              " [37, 9, 38, 39, 6],\n",
              " [37, 9, 38, 39, 6, 40],\n",
              " [37, 9, 38, 39, 6, 40, 10],\n",
              " [37, 9, 38, 39, 6, 40, 10, 15],\n",
              " [37, 9, 38, 39, 6, 40, 10, 15, 41],\n",
              " [37, 9, 38, 39, 6, 40, 10, 15, 41, 3],\n",
              " [37, 9, 38, 39, 6, 40, 10, 15, 41, 3, 42],\n",
              " [37, 9, 38, 39, 6, 40, 10, 15, 41, 3, 42, 5],\n",
              " [37, 9, 38, 39, 6, 40, 10, 15, 41, 3, 42, 5, 43],\n",
              " [16, 10],\n",
              " [16, 10, 15],\n",
              " [16, 10, 15, 44],\n",
              " [16, 10, 15, 44, 45],\n",
              " [16, 10, 15, 44, 45, 1],\n",
              " [16, 10, 15, 44, 45, 1, 46],\n",
              " [16, 10, 15, 44, 45, 1, 46, 47],\n",
              " [16, 10, 15, 44, 45, 1, 46, 47, 2],\n",
              " [16, 10, 15, 44, 45, 1, 46, 47, 2, 17],\n",
              " [16, 10, 15, 44, 45, 1, 46, 47, 2, 17, 7],\n",
              " [16, 10, 15, 44, 45, 1, 46, 47, 2, 17, 7, 1],\n",
              " [16, 10, 15, 44, 45, 1, 46, 47, 2, 17, 7, 1, 18],\n",
              " [16, 10, 15, 44, 45, 1, 46, 47, 2, 17, 7, 1, 18, 48],\n",
              " [1, 49],\n",
              " [1, 49, 50],\n",
              " [1, 49, 50, 51],\n",
              " [1, 49, 50, 51, 52],\n",
              " [1, 49, 50, 51, 52, 2],\n",
              " [1, 49, 50, 51, 52, 2, 53],\n",
              " [1, 49, 50, 51, 52, 2, 53, 3],\n",
              " [1, 49, 50, 51, 52, 2, 53, 3, 54],\n",
              " [1, 49, 50, 51, 52, 2, 53, 3, 54, 55],\n",
              " [1, 49, 50, 51, 52, 2, 53, 3, 54, 55, 2],\n",
              " [1, 49, 50, 51, 52, 2, 53, 3, 54, 55, 2, 56],\n",
              " [1, 49, 50, 51, 52, 2, 53, 3, 54, 55, 2, 56, 9],\n",
              " [1, 49, 50, 51, 52, 2, 53, 3, 54, 55, 2, 56, 9, 57],\n",
              " [1, 49, 50, 51, 52, 2, 53, 3, 54, 55, 2, 56, 9, 57, 58],\n",
              " [1, 49, 50, 51, 52, 2, 53, 3, 54, 55, 2, 56, 9, 57, 58, 8],\n",
              " [1, 49, 50, 51, 52, 2, 53, 3, 54, 55, 2, 56, 9, 57, 58, 8, 59],\n",
              " [1, 49, 50, 51, 52, 2, 53, 3, 54, 55, 2, 56, 9, 57, 58, 8, 59, 3],\n",
              " [1, 49, 50, 51, 52, 2, 53, 3, 54, 55, 2, 56, 9, 57, 58, 8, 59, 3, 60],\n",
              " [19, 12],\n",
              " [19, 12, 61],\n",
              " [19, 12, 61, 20],\n",
              " [19, 12, 61, 20, 1],\n",
              " [19, 12, 61, 20, 1, 62],\n",
              " [19, 12, 61, 20, 1, 62, 63],\n",
              " [19, 12, 61, 20, 1, 62, 63, 9],\n",
              " [19, 12, 61, 20, 1, 62, 63, 9, 64],\n",
              " [19, 12, 61, 20, 1, 62, 63, 9, 64, 12],\n",
              " [19, 12, 61, 20, 1, 62, 63, 9, 64, 12, 1],\n",
              " [19, 12, 61, 20, 1, 62, 63, 9, 64, 12, 1, 65],\n",
              " [19, 12, 61, 20, 1, 62, 63, 9, 64, 12, 1, 65, 66],\n",
              " [19, 12, 61, 20, 1, 62, 63, 9, 64, 12, 1, 65, 66, 9],\n",
              " [19, 12, 61, 20, 1, 62, 63, 9, 64, 12, 1, 65, 66, 9, 67],\n",
              " [19, 12, 61, 20, 1, 62, 63, 9, 64, 12, 1, 65, 66, 9, 67, 68],\n",
              " [5, 69],\n",
              " [5, 69, 2],\n",
              " [5, 69, 2, 21],\n",
              " [5, 69, 2, 21, 70],\n",
              " [5, 69, 2, 21, 70, 4],\n",
              " [5, 69, 2, 21, 70, 4, 71],\n",
              " [5, 69, 2, 21, 70, 4, 71, 72],\n",
              " [5, 69, 2, 21, 70, 4, 71, 72, 22],\n",
              " [5, 69, 2, 21, 70, 4, 71, 72, 22, 1],\n",
              " [5, 69, 2, 21, 70, 4, 71, 72, 22, 1, 73],\n",
              " [5, 69, 2, 21, 70, 4, 71, 72, 22, 1, 73, 23],\n",
              " [5, 69, 2, 21, 70, 4, 71, 72, 22, 1, 73, 23, 74],\n",
              " [4, 75],\n",
              " [4, 75, 10],\n",
              " [4, 75, 10, 2],\n",
              " [4, 75, 10, 2, 76],\n",
              " [4, 75, 10, 2, 76, 77],\n",
              " [4, 75, 10, 2, 76, 77, 1],\n",
              " [4, 75, 10, 2, 76, 77, 1, 24],\n",
              " [4, 75, 10, 2, 76, 77, 1, 24, 7],\n",
              " [4, 75, 10, 2, 76, 77, 1, 24, 7, 78],\n",
              " [4, 75, 10, 2, 76, 77, 1, 24, 7, 78, 79],\n",
              " [4, 75, 10, 2, 76, 77, 1, 24, 7, 78, 79, 13],\n",
              " [4, 75, 10, 2, 76, 77, 1, 24, 7, 78, 79, 13, 80],\n",
              " [4, 75, 10, 2, 76, 77, 1, 24, 7, 78, 79, 13, 80, 81],\n",
              " [4, 75, 10, 2, 76, 77, 1, 24, 7, 78, 79, 13, 80, 81, 11],\n",
              " [4, 75, 10, 2, 76, 77, 1, 24, 7, 78, 79, 13, 80, 81, 11, 82],\n",
              " [83, 84],\n",
              " [83, 84, 7],\n",
              " [83, 84, 7, 85],\n",
              " [83, 84, 7, 85, 86],\n",
              " [83, 84, 7, 85, 86, 4],\n",
              " [83, 84, 7, 85, 86, 4, 87],\n",
              " [83, 84, 7, 85, 86, 4, 87, 25],\n",
              " [83, 84, 7, 85, 86, 4, 87, 25, 88],\n",
              " [83, 84, 7, 85, 86, 4, 87, 25, 88, 1],\n",
              " [83, 84, 7, 85, 86, 4, 87, 25, 88, 1, 89],\n",
              " [83, 84, 7, 85, 86, 4, 87, 25, 88, 1, 89, 90],\n",
              " [83, 84, 7, 85, 86, 4, 87, 25, 88, 1, 89, 90, 2],\n",
              " [83, 84, 7, 85, 86, 4, 87, 25, 88, 1, 89, 90, 2, 91],\n",
              " [83, 84, 7, 85, 86, 4, 87, 25, 88, 1, 89, 90, 2, 91, 92],\n",
              " [83, 84, 7, 85, 86, 4, 87, 25, 88, 1, 89, 90, 2, 91, 92, 93],\n",
              " [83, 84, 7, 85, 86, 4, 87, 25, 88, 1, 89, 90, 2, 91, 92, 93, 23],\n",
              " [94, 95],\n",
              " [94, 95, 5],\n",
              " [94, 95, 5, 13],\n",
              " [94, 95, 5, 13, 2],\n",
              " [94, 95, 5, 13, 2, 26],\n",
              " [94, 95, 5, 13, 2, 26, 3],\n",
              " [94, 95, 5, 13, 2, 26, 3, 96],\n",
              " [94, 95, 5, 13, 2, 26, 3, 96, 4],\n",
              " [94, 95, 5, 13, 2, 26, 3, 96, 4, 97],\n",
              " [94, 95, 5, 13, 2, 26, 3, 96, 4, 97, 6],\n",
              " [94, 95, 5, 13, 2, 26, 3, 96, 4, 97, 6, 1],\n",
              " [94, 95, 5, 13, 2, 26, 3, 96, 4, 97, 6, 1, 98],\n",
              " [94, 95, 5, 13, 2, 26, 3, 96, 4, 97, 6, 1, 98, 99],\n",
              " [94, 95, 5, 13, 2, 26, 3, 96, 4, 97, 6, 1, 98, 99, 100],\n",
              " [94, 95, 5, 13, 2, 26, 3, 96, 4, 97, 6, 1, 98, 99, 100, 101],\n",
              " [94, 95, 5, 13, 2, 26, 3, 96, 4, 97, 6, 1, 98, 99, 100, 101, 19],\n",
              " [102, 103],\n",
              " [102, 103, 7],\n",
              " [102, 103, 7, 104],\n",
              " [102, 103, 7, 104, 27],\n",
              " [102, 103, 7, 104, 27, 8],\n",
              " [102, 103, 7, 104, 27, 8, 105],\n",
              " [106, 107],\n",
              " [106, 107, 3],\n",
              " [106, 107, 3, 108],\n",
              " [106, 107, 3, 108, 109],\n",
              " [106, 107, 3, 108, 109, 22],\n",
              " [106, 107, 3, 108, 109, 22, 1],\n",
              " [106, 107, 3, 108, 109, 22, 1, 110],\n",
              " [106, 107, 3, 108, 109, 22, 1, 110, 111],\n",
              " [106, 107, 3, 108, 109, 22, 1, 110, 111, 28],\n",
              " [106, 107, 3, 108, 109, 22, 1, 110, 111, 28, 2],\n",
              " [106, 107, 3, 108, 109, 22, 1, 110, 111, 28, 2, 112],\n",
              " [113, 6],\n",
              " [113, 6, 114],\n",
              " [113, 6, 114, 2],\n",
              " [113, 6, 114, 2, 26],\n",
              " [113, 6, 114, 2, 26, 3],\n",
              " [113, 6, 114, 2, 26, 3, 115],\n",
              " [113, 6, 114, 2, 26, 3, 115, 27],\n",
              " [113, 6, 114, 2, 26, 3, 115, 27, 116],\n",
              " [113, 6, 114, 2, 26, 3, 115, 27, 116, 117],\n",
              " [113, 6, 114, 2, 26, 3, 115, 27, 116, 117, 4],\n",
              " [113, 6, 114, 2, 26, 3, 115, 27, 116, 117, 4, 118],\n",
              " [113, 6, 114, 2, 26, 3, 115, 27, 116, 117, 4, 118, 119],\n",
              " [113, 6, 114, 2, 26, 3, 115, 27, 116, 117, 4, 118, 119, 1],\n",
              " [113, 6, 114, 2, 26, 3, 115, 27, 116, 117, 4, 118, 119, 1, 24],\n",
              " [7, 1],\n",
              " [7, 1, 120],\n",
              " [7, 1, 120, 13],\n",
              " [7, 1, 120, 13, 18],\n",
              " [7, 1, 120, 13, 18, 121],\n",
              " [7, 1, 120, 13, 18, 121, 1],\n",
              " [7, 1, 120, 13, 18, 121, 1, 8],\n",
              " [7, 1, 120, 13, 18, 121, 1, 8, 122],\n",
              " [7, 1, 120, 13, 18, 121, 1, 8, 122, 4],\n",
              " [7, 1, 120, 13, 18, 121, 1, 8, 122, 4, 123],\n",
              " [7, 1, 120, 13, 18, 121, 1, 8, 122, 4, 123, 3],\n",
              " [7, 1, 120, 13, 18, 121, 1, 8, 122, 4, 123, 3, 25],\n",
              " [7, 1, 120, 13, 18, 121, 1, 8, 122, 4, 123, 3, 25, 124],\n",
              " [7, 1, 120, 13, 18, 121, 1, 8, 122, 4, 123, 3, 25, 124, 125],\n",
              " [7, 1, 120, 13, 18, 121, 1, 8, 122, 4, 123, 3, 25, 124, 125, 1],\n",
              " [7, 1, 120, 13, 18, 121, 1, 8, 122, 4, 123, 3, 25, 124, 125, 1, 126],\n",
              " [127, 1],\n",
              " [127, 1, 128],\n",
              " [127, 1, 128, 3],\n",
              " [127, 1, 128, 3, 129],\n",
              " [127, 1, 128, 3, 129, 130],\n",
              " [127, 1, 128, 3, 129, 130, 131],\n",
              " [127, 1, 128, 3, 129, 130, 131, 132],\n",
              " [127, 1, 128, 3, 129, 130, 131, 132, 12],\n",
              " [127, 1, 128, 3, 129, 130, 131, 132, 12, 133],\n",
              " [127, 1, 128, 3, 129, 130, 131, 132, 12, 133, 10],\n",
              " [127, 1, 128, 3, 129, 130, 131, 132, 12, 133, 10, 134],\n",
              " [127, 1, 128, 3, 129, 130, 131, 132, 12, 133, 10, 134, 135],\n",
              " [127, 1, 128, 3, 129, 130, 131, 132, 12, 133, 10, 134, 135, 136],\n",
              " [127, 1, 128, 3, 129, 130, 131, 132, 12, 133, 10, 134, 135, 136, 137],\n",
              " [127, 1, 128, 3, 129, 130, 131, 132, 12, 133, 10, 134, 135, 136, 137, 138],\n",
              " [127, 1, 128, 3, 129, 130, 131, 132, 12, 133, 10, 134, 135, 136, 137, 138, 4],\n",
              " [139, 5],\n",
              " [139, 5, 140],\n",
              " [139, 5, 140, 141],\n",
              " [139, 5, 140, 141, 142],\n",
              " [139, 5, 140, 141, 142, 143],\n",
              " [139, 5, 140, 141, 142, 143, 144],\n",
              " [139, 5, 140, 141, 142, 143, 144, 8],\n",
              " [139, 5, 140, 141, 142, 143, 144, 8, 145],\n",
              " [139, 5, 140, 141, 142, 143, 144, 8, 145, 146],\n",
              " [139, 5, 140, 141, 142, 143, 144, 8, 145, 146, 9],\n",
              " [139, 5, 140, 141, 142, 143, 144, 8, 145, 146, 9, 147],\n",
              " [139, 5, 140, 141, 142, 143, 144, 8, 145, 146, 9, 147, 1],\n",
              " [139, 5, 140, 141, 142, 143, 144, 8, 145, 146, 9, 147, 1, 16],\n",
              " [6, 8],\n",
              " [6, 8, 148],\n",
              " [6, 8, 148, 3],\n",
              " [6, 8, 148, 3, 149],\n",
              " [6, 8, 148, 3, 149, 11],\n",
              " [6, 8, 148, 3, 149, 11, 150],\n",
              " [6, 8, 148, 3, 149, 11, 150, 151],\n",
              " [6, 8, 148, 3, 149, 11, 150, 151, 2],\n",
              " [6, 8, 148, 3, 149, 11, 150, 151, 2, 17],\n",
              " [6, 8, 148, 3, 149, 11, 150, 151, 2, 17, 7],\n",
              " [6, 8, 148, 3, 149, 11, 150, 151, 2, 17, 7, 1],\n",
              " [6, 8, 148, 3, 149, 11, 150, 151, 2, 17, 7, 1, 152],\n",
              " [6, 8, 148, 3, 149, 11, 150, 151, 2, 17, 7, 1, 152, 14],\n",
              " [6, 8, 148, 3, 149, 11, 150, 151, 2, 17, 7, 1, 152, 14, 3],\n",
              " [153, 154],\n",
              " [153, 154, 4],\n",
              " [153, 154, 4, 155],\n",
              " [153, 154, 4, 155, 5],\n",
              " [153, 154, 4, 155, 5, 156],\n",
              " [153, 154, 4, 155, 5, 156, 20],\n",
              " [153, 154, 4, 155, 5, 156, 20, 8],\n",
              " [153, 154, 4, 155, 5, 156, 20, 8, 157],\n",
              " [153, 154, 4, 155, 5, 156, 20, 8, 157, 158],\n",
              " [153, 154, 4, 155, 5, 156, 20, 8, 157, 158, 6],\n",
              " [153, 154, 4, 155, 5, 156, 20, 8, 157, 158, 6, 159],\n",
              " [153, 154, 4, 155, 5, 156, 20, 8, 157, 158, 6, 159, 160],\n",
              " [28, 2],\n",
              " [28, 2, 21],\n",
              " [28, 2, 21, 161],\n",
              " [28, 2, 21, 161, 6],\n",
              " [28, 2, 21, 161, 6, 2],\n",
              " [28, 2, 21, 161, 6, 2, 162],\n",
              " [28, 2, 21, 161, 6, 2, 162, 163],\n",
              " [28, 2, 21, 161, 6, 2, 162, 163, 164],\n",
              " [28, 2, 21, 161, 6, 2, 162, 163, 164, 4],\n",
              " [28, 2, 21, 161, 6, 2, 162, 163, 164, 4, 165]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the size of above output is unequal and here we are performing supervised so we need to perform padding here"
      ],
      "metadata": {
        "id": "5RvAPaHoe1Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=max([len(x) for x in input_sequences])"
      ],
      "metadata": {
        "id": "YoKK0uKOgLD5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "padded_input_sequences=pad_sequences(input_sequences,maxlen=max_len,padding='pre')"
      ],
      "metadata": {
        "id": "90zmNNDienxN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt0gy512g2_l",
        "outputId": "a088f336-be24-4f31-c582-561678bfadfd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,  29,   5],\n",
              "       [  0,   0,   0, ...,  29,   5,   2],\n",
              "       [  0,   0,   0, ...,   5,   2,  30],\n",
              "       ...,\n",
              "       [  0,   0,   0, ..., 162, 163, 164],\n",
              "       [  0,   0,   0, ..., 163, 164,   4],\n",
              "       [  0,   0,   0, ..., 164,   4, 165]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=padded_input_sequences[:,:-1]"
      ],
      "metadata": {
        "id": "rLi0JKiig6c1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0JNA5iphVNV",
        "outputId": "0be7bbdc-ecb5-4df4-f7a3-4703d7b0e2c3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,   0,  29],\n",
              "       [  0,   0,   0, ...,   0,  29,   5],\n",
              "       [  0,   0,   0, ...,  29,   5,   2],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,   2, 162, 163],\n",
              "       [  0,   0,   0, ..., 162, 163, 164],\n",
              "       [  0,   0,   0, ..., 163, 164,   4]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=padded_input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "oYhog49fhVuL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2tvoOevhazF",
        "outputId": "f7d63c40-8598-4d3f-ce0d-efc024d160ea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  5,   2,  30,  14,   3,  31,   4,  32,  33,  34,  11,  35,   2,\n",
              "        36,   9,  38,  39,   6,  40,  10,  15,  41,   3,  42,   5,  43,\n",
              "        10,  15,  44,  45,   1,  46,  47,   2,  17,   7,   1,  18,  48,\n",
              "        49,  50,  51,  52,   2,  53,   3,  54,  55,   2,  56,   9,  57,\n",
              "        58,   8,  59,   3,  60,  12,  61,  20,   1,  62,  63,   9,  64,\n",
              "        12,   1,  65,  66,   9,  67,  68,  69,   2,  21,  70,   4,  71,\n",
              "        72,  22,   1,  73,  23,  74,  75,  10,   2,  76,  77,   1,  24,\n",
              "         7,  78,  79,  13,  80,  81,  11,  82,  84,   7,  85,  86,   4,\n",
              "        87,  25,  88,   1,  89,  90,   2,  91,  92,  93,  23,  95,   5,\n",
              "        13,   2,  26,   3,  96,   4,  97,   6,   1,  98,  99, 100, 101,\n",
              "        19, 103,   7, 104,  27,   8, 105, 107,   3, 108, 109,  22,   1,\n",
              "       110, 111,  28,   2, 112,   6, 114,   2,  26,   3, 115,  27, 116,\n",
              "       117,   4, 118, 119,   1,  24,   1, 120,  13,  18, 121,   1,   8,\n",
              "       122,   4, 123,   3,  25, 124, 125,   1, 126,   1, 128,   3, 129,\n",
              "       130, 131, 132,  12, 133,  10, 134, 135, 136, 137, 138,   4,   5,\n",
              "       140, 141, 142, 143, 144,   8, 145, 146,   9, 147,   1,  16,   8,\n",
              "       148,   3, 149,  11, 150, 151,   2,  17,   7,   1, 152,  14,   3,\n",
              "       154,   4, 155,   5, 156,  20,   8, 157, 158,   6, 159, 160,   2,\n",
              "        21, 161,   6,   2, 162, 163, 164,   4, 165], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chaQHxw5hbsn",
        "outputId": "bc008862-d09a-4331-ab98-0b7dbb56246e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(243, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JeK9Jxlipl6",
        "outputId": "fd49d777-4427-4ec1-b82f-ccf9d70e90fe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(243,)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y=to_categorical(y,num_classes=166)"
      ],
      "metadata": {
        "id": "-yKbqKglirio"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuBIjlDwjSVq",
        "outputId": "b985f336-4437-4d62-c0e3-d664319692f7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(243, 166)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPfCisH9jTmD",
        "outputId": "0ece5271-ce17-439f-fb68-df63e19a12bc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "DHMpka_GjWUP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(166,100,input_length=18))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(166, activation='softmax'))"
      ],
      "metadata": {
        "id": "9poRSMMRUebX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "1IZGrhM2Viud"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-7U8rIQV38V",
        "outputId": "d069d0d9-39c7-4e7a-d0d1-12189b8105ab"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 18, 100)           16600     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 150)               150600    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 166)               25066     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 192266 (751.04 KB)\n",
            "Trainable params: 192266 (751.04 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x,y,epochs=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73OoTAHtV6Ck",
        "outputId": "d7c3e16e-f2db-49e0-a825-9b3a0a7efe6c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/90\n",
            "8/8 [==============================] - 0s 40ms/step - loss: 0.0499 - accuracy: 1.0000\n",
            "Epoch 2/90\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 0.0494 - accuracy: 1.0000\n",
            "Epoch 3/90\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0484 - accuracy: 1.0000\n",
            "Epoch 4/90\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.0475 - accuracy: 1.0000\n",
            "Epoch 5/90\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.0468 - accuracy: 1.0000\n",
            "Epoch 6/90\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.0462 - accuracy: 1.0000\n",
            "Epoch 7/90\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.0450 - accuracy: 1.0000\n",
            "Epoch 8/90\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.0446 - accuracy: 1.0000\n",
            "Epoch 9/90\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.0440 - accuracy: 1.0000\n",
            "Epoch 10/90\n",
            "8/8 [==============================] - 0s 37ms/step - loss: 0.0429 - accuracy: 1.0000\n",
            "Epoch 11/90\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.0421 - accuracy: 1.0000\n",
            "Epoch 12/90\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.0415 - accuracy: 1.0000\n",
            "Epoch 13/90\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0408 - accuracy: 1.0000\n",
            "Epoch 14/90\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0401 - accuracy: 1.0000\n",
            "Epoch 15/90\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.0395 - accuracy: 1.0000\n",
            "Epoch 16/90\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.0389 - accuracy: 1.0000\n",
            "Epoch 17/90\n",
            "8/8 [==============================] - 0s 49ms/step - loss: 0.0384 - accuracy: 1.0000\n",
            "Epoch 18/90\n",
            "8/8 [==============================] - 1s 77ms/step - loss: 0.0376 - accuracy: 1.0000\n",
            "Epoch 19/90\n",
            "8/8 [==============================] - 1s 84ms/step - loss: 0.0371 - accuracy: 1.0000\n",
            "Epoch 20/90\n",
            "8/8 [==============================] - 0s 64ms/step - loss: 0.0366 - accuracy: 1.0000\n",
            "Epoch 21/90\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.0360 - accuracy: 1.0000\n",
            "Epoch 22/90\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0354 - accuracy: 1.0000\n",
            "Epoch 23/90\n",
            "8/8 [==============================] - 0s 37ms/step - loss: 0.0351 - accuracy: 1.0000\n",
            "Epoch 24/90\n",
            "8/8 [==============================] - 0s 62ms/step - loss: 0.0345 - accuracy: 1.0000\n",
            "Epoch 25/90\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.0340 - accuracy: 1.0000\n",
            "Epoch 26/90\n",
            "8/8 [==============================] - 0s 41ms/step - loss: 0.0335 - accuracy: 1.0000\n",
            "Epoch 27/90\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0330 - accuracy: 1.0000\n",
            "Epoch 28/90\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0325 - accuracy: 1.0000\n",
            "Epoch 29/90\n",
            "8/8 [==============================] - 1s 100ms/step - loss: 0.0320 - accuracy: 1.0000\n",
            "Epoch 30/90\n",
            "8/8 [==============================] - 1s 75ms/step - loss: 0.0315 - accuracy: 1.0000\n",
            "Epoch 31/90\n",
            "8/8 [==============================] - 1s 74ms/step - loss: 0.0311 - accuracy: 1.0000\n",
            "Epoch 32/90\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.0309 - accuracy: 1.0000\n",
            "Epoch 33/90\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.0303 - accuracy: 1.0000\n",
            "Epoch 34/90\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.0300 - accuracy: 1.0000\n",
            "Epoch 35/90\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.0296 - accuracy: 1.0000\n",
            "Epoch 36/90\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0290 - accuracy: 1.0000\n",
            "Epoch 37/90\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.0286 - accuracy: 1.0000\n",
            "Epoch 38/90\n",
            "8/8 [==============================] - 0s 37ms/step - loss: 0.0282 - accuracy: 1.0000\n",
            "Epoch 39/90\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0277 - accuracy: 1.0000\n",
            "Epoch 40/90\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.0274 - accuracy: 1.0000\n",
            "Epoch 41/90\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.0271 - accuracy: 1.0000\n",
            "Epoch 42/90\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0267 - accuracy: 1.0000\n",
            "Epoch 43/90\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0265 - accuracy: 1.0000\n",
            "Epoch 44/90\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.0262 - accuracy: 1.0000\n",
            "Epoch 45/90\n",
            "8/8 [==============================] - 0s 41ms/step - loss: 0.0257 - accuracy: 1.0000\n",
            "Epoch 46/90\n",
            "8/8 [==============================] - 0s 58ms/step - loss: 0.0253 - accuracy: 1.0000\n",
            "Epoch 47/90\n",
            "8/8 [==============================] - 0s 57ms/step - loss: 0.0250 - accuracy: 1.0000\n",
            "Epoch 48/90\n",
            "8/8 [==============================] - 0s 59ms/step - loss: 0.0248 - accuracy: 1.0000\n",
            "Epoch 49/90\n",
            "8/8 [==============================] - 0s 55ms/step - loss: 0.0245 - accuracy: 1.0000\n",
            "Epoch 50/90\n",
            "8/8 [==============================] - 0s 51ms/step - loss: 0.0243 - accuracy: 1.0000\n",
            "Epoch 51/90\n",
            "8/8 [==============================] - 0s 46ms/step - loss: 0.0241 - accuracy: 1.0000\n",
            "Epoch 52/90\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 0.0237 - accuracy: 1.0000\n",
            "Epoch 53/90\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 0.0232 - accuracy: 1.0000\n",
            "Epoch 54/90\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 0.0230 - accuracy: 1.0000\n",
            "Epoch 55/90\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 0.0227 - accuracy: 1.0000\n",
            "Epoch 56/90\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 0.0224 - accuracy: 1.0000\n",
            "Epoch 57/90\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.0221 - accuracy: 1.0000\n",
            "Epoch 58/90\n",
            "8/8 [==============================] - 0s 42ms/step - loss: 0.0219 - accuracy: 1.0000\n",
            "Epoch 59/90\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.0217 - accuracy: 1.0000\n",
            "Epoch 60/90\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.0214 - accuracy: 1.0000\n",
            "Epoch 61/90\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0211 - accuracy: 1.0000\n",
            "Epoch 62/90\n",
            "8/8 [==============================] - 1s 80ms/step - loss: 0.0209 - accuracy: 1.0000\n",
            "Epoch 63/90\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0207 - accuracy: 1.0000\n",
            "Epoch 64/90\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0204 - accuracy: 1.0000\n",
            "Epoch 65/90\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0202 - accuracy: 1.0000\n",
            "Epoch 66/90\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0200 - accuracy: 1.0000\n",
            "Epoch 67/90\n",
            "8/8 [==============================] - 0s 37ms/step - loss: 0.0197 - accuracy: 1.0000\n",
            "Epoch 68/90\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0195 - accuracy: 1.0000\n",
            "Epoch 69/90\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.0193 - accuracy: 1.0000\n",
            "Epoch 70/90\n",
            "8/8 [==============================] - 0s 45ms/step - loss: 0.0190 - accuracy: 1.0000\n",
            "Epoch 71/90\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0188 - accuracy: 1.0000\n",
            "Epoch 72/90\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0186 - accuracy: 1.0000\n",
            "Epoch 73/90\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.0185 - accuracy: 1.0000\n",
            "Epoch 74/90\n",
            "8/8 [==============================] - 0s 37ms/step - loss: 0.0183 - accuracy: 1.0000\n",
            "Epoch 75/90\n",
            "8/8 [==============================] - 0s 40ms/step - loss: 0.0181 - accuracy: 1.0000\n",
            "Epoch 76/90\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.0179 - accuracy: 1.0000\n",
            "Epoch 77/90\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.0177 - accuracy: 1.0000\n",
            "Epoch 78/90\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.0175 - accuracy: 1.0000\n",
            "Epoch 79/90\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.0174 - accuracy: 1.0000\n",
            "Epoch 80/90\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.0171 - accuracy: 1.0000\n",
            "Epoch 81/90\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.0170 - accuracy: 1.0000\n",
            "Epoch 82/90\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.0168 - accuracy: 1.0000\n",
            "Epoch 83/90\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.0166 - accuracy: 1.0000\n",
            "Epoch 84/90\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.0165 - accuracy: 1.0000\n",
            "Epoch 85/90\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0162 - accuracy: 1.0000\n",
            "Epoch 86/90\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.0161 - accuracy: 1.0000\n",
            "Epoch 87/90\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0160 - accuracy: 1.0000\n",
            "Epoch 88/90\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.0158 - accuracy: 1.0000\n",
            "Epoch 89/90\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.0156 - accuracy: 1.0000\n",
            "Epoch 90/90\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0155 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e158144dba0>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "v3FQfOw1YtX9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Her beautiful locks\"\n",
        "for i in range(10):\n",
        "   #tokenize\n",
        "   token_text=tokenizer.texts_to_sequences([text])[0]\n",
        "   #padding\n",
        "   padded_token_test=pad_sequences([token_text],maxlen=18,padding='pre')\n",
        "   #predict\n",
        "   pos=np.argmax(model.predict(padded_token_test))\n",
        "   for word,index in tokenizer.word_index.items():\n",
        "     if index==pos:\n",
        "      text=text+\" \"+word\n",
        "      print(text)\n",
        "      time.sleep(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2xXxi4PXZS9",
        "outputId": "86489fb2-fef9-4743-9c2b-ed33657ec758"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "Her beautiful locks cascade\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Her beautiful locks cascade like\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Her beautiful locks cascade like a\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "Her beautiful locks cascade like a waterfall\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Her beautiful locks cascade like a waterfall of\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Her beautiful locks cascade like a waterfall of silk\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Her beautiful locks cascade like a waterfall of silk framing\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Her beautiful locks cascade like a waterfall of silk framing a\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "Her beautiful locks cascade like a waterfall of silk framing a smile\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Her beautiful locks cascade like a waterfall of silk framing a smile that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SHxAwx1OZpJO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}